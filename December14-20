import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
try:
	import statsmodels.api as sm
except Exception:
	sm = None
	warnings.warn("statsmodels is not installed; continuing without it")

from scipy import stats
from scipy.stats import ttest_1samp
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats.mstats import winsorize
from scipy.stats import boxcox
from sklearn.model_selection import KFold, LeaveOneOut, cross_val_score, train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import ConfusionMatrixDisplay, mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.linear_model import Lasso, Ridge, ElasticNet
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, auc, roc_curve)
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

print("===================== DECEMBER - 14 =====================")

df = pd.read_csv('Breast_cancer_dataset.csv')
df.sample(5)
print(df)
print(df.info())    
print(df.describe())
df.isnull().sum()
df.drop(['id','Unnamed: 32'],axis=1,inplace=True)
print(df.info())

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,
                             precision_score, recall_score, f1_score, auc, roc_curve)
#df = pd.read_csv('/content/Breast_cancer_dataset.csv')
df.sample(5)
df.info()
df.describe()
df.shape
df.isnull().sum()
#df['Unnamed: 32'].unique()
#df = df.drop(['id', 'Unnamed: 32'], axis=1)
df.sample(2)
df['diagnosis'].unique()
sns.countplot(x="diagnosis",data=df)
plt.title('Count Plot- B:Benign & M:Malignant')
plt.xlabel('Diagnosis')
plt.ylabel('Count')
#plt.show()
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['diagnosis'] = le.fit_transform(df['diagnosis'])
x = df.drop('diagnosis', axis=1)
y = df['diagnosis']
#train & test for x & y
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
import warnings
warnings.filterwarnings('ignore')
##**Logistic Regression**
lor = LogisticRegression()
lor.fit(x_train,y_train)
y_pred = lor.predict(x_test)
print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
print(precision_score(y_test,y_pred, pos_label=0))
print(precision_score(y_test,y_pred, pos_label=0))
print(recall_score(y_test,y_pred, pos_label=1))
print(recall_score(y_test,y_pred, pos_label=0))
print(f1_score(y_test,y_pred, pos_label=1))
print(f1_score(y_test,y_pred, pos_label=0))
print(classification_report(y_test,y_pred))
pipeline_lor = Pipeline([('scaler', StandardScaler()), ('lor', LogisticRegression())])
pipeline_lor.fit(x_train, y_train)
y_pred_pipeline_lor = pipeline_lor.predict(x_test)
acc_pipeline_lor = accuracy_score(y_test, y_pred_pipeline_lor)
print("Accuracy with Pipeline:", acc_pipeline_lor)
print(classification_report(y_test,y_pred_pipeline_lor))
from sklearn.metrics import ConfusionMatrixDisplay
print(confusion_matrix(y_test, y_pred_pipeline_lor))
cm_lor = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_pipeline_lor), display_labels=pipeline_lor.classes_)
cm_lor.plot()
#plt.show()
y_pred_test_prob = pipeline_lor.predict_proba(x_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred_test_prob)
roc_auc = auc(fpr, tpr) #x,y -> fpr, tpr
roc_auc*100
plt.plot(fpr, tpr, color='orange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--', lw=2, color='grey')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
#plt.show()
##**Naive Bayes**
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
pipeline_gnb = Pipeline([('scaler', StandardScaler()), ('gnb', GaussianNB())])
pipeline_gnb.fit(x_train, y_train)
y_pred_train_gnb = pipeline_gnb.predict(x_train)
y_pred_test_gnb = pipeline_gnb.predict(x_test)
print(y_pred_test_gnb)
print(y_pred_test_gnb)
acc_train_gnb = accuracy_score(y_train, y_pred_train_gnb)
acc_test_gnb = accuracy_score(y_test, y_pred_test_gnb)
print('Train: ', acc_train_gnb*100)
print('Test: ',acc_test_gnb*100)
#Precision, Recall, F1 Score, Confusion Matrix
print('Precision: ', precision_score(y_test, y_pred_test_gnb))
#precision_score(y_test, y_pred_test_gnb, pos_label='M')
#precision_score(y_test, y_pred_test_gnb, pos_label='B')
print('Recall: ', recall_score(y_test, y_pred_test_gnb))
print('F1 Score: ', f1_score(y_test, y_pred_test_gnb))
print(classification_report(y_test, y_pred_test_gnb))
print(confusion_matrix(y_test,y_pred_test_gnb))
cm_gnb = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test_gnb), display_labels = ['B', 'M'])
cm_gnb.plot()
#plt.show()
y_pred_test_prob_gnb = pipeline_gnb.predict_proba(x_test)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_pred_test_prob_gnb)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, color='orange', lw=2, label='ROC curve')
plt.plot([0, 1], [0, 1], 'k--', lw=2, color='grey')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
#plt.show()
from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB
pipeline_bnb = Pipeline ([('scaler', StandardScaler()), ('bnb',BernoulliNB())])
pipeline_bnb.fit(x_train,y_train)
y_pred_train_bnb = pipeline_bnb.predict(x_train)
y_pred_test_bnb = pipeline_bnb.predict(x_test)
acc_train_bnb = accuracy_score(y_train, y_pred_train_bnb)
acc_test_bnb = accuracy_score(y_test, y_pred_test_bnb)
print(acc_train_bnb*100)
print(acc_test_bnb*100)
print(classification_report(y_test, y_pred_test_bnb))
print(confusion_matrix(y_test,y_pred_test_bnb))
cm_gnb = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test_bnb), display_labels = ['B', 'M'])
cm_gnb.plot()
#plt.show()
y_pred_test_prob_bnb = pipeline_bnb.predict_proba(x_test)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_pred_test_prob_bnb)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, color='orange', lw=2, label='ROC curve')
plt.plot([0, 1], [0, 1], 'k--', lw=2, color='grey')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
#plt.show()
from sklearn.preprocessing import MinMaxScaler
pipeline_mnb = Pipeline([('scaler', MinMaxScaler()), ('mnb',MultinomialNB())])
pipeline_mnb.fit(x_train,y_train)
y_pred_train_mnb = pipeline_mnb.predict(x_train)
y_pred_test_mnb = pipeline_mnb.predict(x_test)
acc_train_mnb = accuracy_score(y_train, y_pred_train_mnb)
acc_test_mnb = accuracy_score(y_test, y_pred_test_mnb)
print('Train: ', acc_train_mnb*100)
print('Test: ', acc_test_mnb*100)
print(precision_score(y_test,y_pred_test_mnb))
print(recall_score(y_test,y_pred_test_mnb))
print(f1_score(y_test,y_pred_test_mnb))
y_pred_test_prob_mnb = pipeline_mnb.predict_proba(x_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_prob_mnb)
roc_auc = auc(fpr,tpr) # x,y ->fpr,tpr
plt.plot(fpr, tpr, color='orange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--', lw=2, color='grey')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
#plt.show()
##**K Nearest neighbors**
from sklearn.neighbors import KNeighborsClassifier
pipeline_knn = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])
pipeline_knn.fit(x_train, y_train)
y_pred_train_knn = pipeline_knn.predict(x_train)
y_pred_test_knn = pipeline_knn.predict(x_test)
training_accuracy = accuracy_score(y_train, y_pred_train_knn)
testing_accuracy = accuracy_score(y_test, y_pred_test_knn)
print("\nK-Nearest Neighbors (KNN)")
print(f"Training Accuracy: {training_accuracy}")
print(f"Testing Accuracy: {testing_accuracy}")
y_pred_prob_knn = pipeline_knn.predict_proba(x_test)[:, 1]
conf_matrix_knn = confusion_matrix(y_test, y_pred_test_knn)
print("Confusion Matrix:")
print(conf_matrix_knn)
cm_display = ConfusionMatrixDisplay(confusion_matrix = conf_matrix_knn,
display_labels = ["benign", "malignant"])
cm_display.plot()
#plt.show()
print(classification_report(y_test, y_pred_test_knn))
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_knn)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
#plt.show()
from sklearn.model_selection import GridSearchCV

param_grid = {'knn__n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]} #np.arange(1, 21,1)
grid_search = GridSearchCV(pipeline_knn, param_grid, cv=5)
grid_search.fit(x_train, y_train)
optimal_k = grid_search.best_params_['knn__n_neighbors']
print("Optimal K:", optimal_k)
print("Optimal Score", grid_search.best_score_)
plt.figure(figsize=(10, 6))
plt.plot(param_grid['knn__n_neighbors'], grid_search.cv_results_['mean_test_score'], marker='*')
plt.xlabel('K Neighbors')
plt.ylabel('Mean Test Score')
#plt.show()
##**Decision Tree**
from sklearn.tree import DecisionTreeClassifier
pipeline_dt = Pipeline([
('scaler', StandardScaler()),
('dt', DecisionTreeClassifier())
])
pipeline_dt.fit(x_train, y_train)
y_pred_train_dt = pipeline_dt.predict(x_train)
y_pred_test_dt = pipeline_dt.predict(x_test)
training_accuracy = accuracy_score(y_train, y_pred_train_dt)
testing_accuracy = accuracy_score(y_test, y_pred_test_dt)
print("\nDecision Trees")
print(f"Training Accuracy: {training_accuracy}")
print(f"Testing Accuracy: {testing_accuracy}")
y_pred_prob_dt = pipeline_dt.predict_proba(x_test)[:, 1]
conf_matrix_dt = confusion_matrix(y_test, y_pred_test_dt)
print("Confusion Matrix:")
print(conf_matrix_dt)
cm_display = ConfusionMatrixDisplay(confusion_matrix = conf_matrix_dt, display_labels =["benign", "malignant"])
cm_display.plot()
#plt.show()
print(classification_report(y_test, y_pred_test_dt))
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_dt)
roc_auc = auc(fpr, tpr)

youden_j = tpr - fpr
optimal_idx = np.argmax(youden_j)
optimal_threshold = thresholds[optimal_idx]
optimal_fpr = fpr[optimal_idx]
optimal_tpr = tpr[optimal_idx]

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.scatter(optimal_fpr, optimal_tpr, color='red', marker='o', s=100, label=f'Optimal Threshold: {optimal_threshold:.2f}')
plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
#plt.show()
from sklearn.model_selection import cross_val_score, GridSearchCV
cv_scores = cross_val_score(pipeline_dt, x_train, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", cv_scores.mean())
pipeline_dt_tuned = Pipeline([('scaler', StandardScaler()),('dt', DecisionTreeClassifier())])
param_grid = {
    'dt__ccp_alpha': [0.0, 0.01, 0.05, 0.1],
    'dt__max_depth': [None, 10, 20, 30],
    'dt__min_samples_split': [2, 5, 10],
    'dt__min_samples_leaf' : [1, 2, 4, 6, 8, 10],
    'dt__max_features': ['auto', 'sqrt', 'log2'],
    'dt__criterion': ['gini', 'entropy']
}
grid_search = GridSearchCV(pipeline_dt_tuned, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_train, y_train)
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Cross-Validation Score: {grid_search.best_score_}")
y_pred = grid_search.best_estimator_.predict(x_test)
testing_accuracy = accuracy_score(y_test, y_pred)
print(f"Testing Accuracy: {testing_accuracy}")
print(classification_report(y_test, y_pred))
conf_matrix_dt_tuned = confusion_matrix(y_test, y_pred)
print("Confusion Matrix for Tuned Decision Tree:")
print(conf_matrix_dt_tuned)

cm_display_tuned = ConfusionMatrixDisplay(confusion_matrix = conf_matrix_dt_tuned,
display_labels = grid_search.best_estimator_.named_steps['dt'].classes_)
cm_display_tuned.plot()
plt.title('Confusion Matrix - Tuned Decision Tree')
#plt.show()
##**Support Vector Machine**
from sklearn.svm import SVC
pipeline_svm = Pipeline([('scaler', StandardScaler()), ('svm', SVC(probability=True))])
pipeline_svm.fit(x_train, y_train)
y_pred_train_svm = pipeline_svm.predict(x_train)
y_pred_test_svm = pipeline_svm.predict(x_test)
training_accuracy = accuracy_score(y_train, y_pred_train_svm)
testing_accuracy = accuracy_score(y_test, y_pred_test_svm)
print("\nSupport Vector Machines (SVM)")
print(f"Training Accuracy: {training_accuracy}")
print(f"Testing Accuracy: {testing_accuracy}")
y_pred_prob_svm = pipeline_svm.predict_proba(x_test)[:, 1]
conf_matrix_svm = confusion_matrix(y_test, y_pred_test_svm)
print("Confusion Matrix:")
print(conf_matrix_svm)
cm_display = ConfusionMatrixDisplay(confusion_matrix = conf_matrix_svm, display_labels = ["benign", "malignant"])
cm_display.plot()
#plt.show()
print(classification_report(y_test, y_pred_test_svm))
y_pred_prob_svm = pipeline_svm.decision_function(x_test)
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_svm)
roc_auc = auc(fpr, tpr)
youden_j = tpr - fpr
optimal_threshold_index = np.argmax(youden_j)
optimal_threshold = thresholds[optimal_threshold_index]
print(f"Optimal Threshold: {optimal_threshold:.4f}")
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.scatter(fpr[optimal_threshold_index], tpr[optimal_threshold_index], color='red',
            marker='o', label=f'Optimal Threshold = {optimal_threshold:.4f}')
plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - SVM')
plt.legend(loc='lower right')
#plt.show()



print("===================== DECEMBER - 20 =====================")
