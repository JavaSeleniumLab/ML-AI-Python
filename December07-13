
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
try:
	import statsmodels.api as sm
except Exception:
	sm = None
	warnings.warn("statsmodels is not installed; continuing without it")

from scipy import stats
from scipy.stats import ttest_1samp
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats.mstats import winsorize
from scipy.stats import boxcox
from sklearn.model_selection import KFold, LeaveOneOut, cross_val_score, train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import ConfusionMatrixDisplay, mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.linear_model import Lasso, Ridge, ElasticNet
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, auc, roc_curve)
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

print("===================== DECEMBER -07 =====================")



df = pd.read_csv('Breast_cancer_dataset.csv')
df.sample(5)
print(df)
print(df.info())    
print(df.describe())
df.isnull().sum()
df.drop(['id','Unnamed: 32'],axis=1,inplace=True)
print(df.info())    
print(df.sample(2))
sns.countplot(df['diagnosis'])
#plt.show()
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['diagnosis'] = le.fit_transform(df['diagnosis'])
x = df.drop('diagnosis',axis=1)
y = df['diagnosis']

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)
print(x_train)
print(x_test)
print("==========================================")

# OneHotEncoder = OneHotEncoder()
# y_train = y_train.replace({'M':1,'B':0})
lor = LogisticRegression()
lor.fit(x_train, y_train)
y_pred = lor.predict(x_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=['Benign', 'Malignant']))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred, labels=lor.classes_))

print("Precision M :", precision_score(y_test, y_pred))
print("Precision B :", precision_score(y_test, y_pred,  pos_label=1))

print("Recall M :", recall_score(y_test, y_pred,  pos_label=0))
print("Recall B :", recall_score(y_test, y_pred,  pos_label=1))

print("F1 Score M :", f1_score(y_test, y_pred,  pos_label=0))
print("F1 Score B :", f1_score(y_test, y_pred,  pos_label=1))

print("==========================================")
print(classification_report(y_test, y_pred))


print("================PIPELINE===================")
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('logistic_regression', LogisticRegression())
])
pipeline.fit(x_train, y_train)
y_pred_pipeline = pipeline.predict(x_test)
print("Accuracy (Pipeline):", accuracy_score(y_test, y_pred_pipeline))
print("==========================================")
print("Classification Report (Pipeline):\n", classification_report(y_test, y_pred_pipeline))
print("Confusion Matrix (Pipeline):\n", confusion_matrix(y_test, y_pred_pipeline))
print("==========================================")
y_pred_pipeline_lor = pipeline.predict(x_test)
print("Accuracy (Pipeline with Logistic Regression):", accuracy_score(y_test, y_pred_pipeline_lor))
print("==========================================")
print(classification_report(y_test,y_pred_pipeline_lor))
print("===================================")
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix
print(confusion_matrix(y_test, y_pred_pipeline_lor))
cm_lor=ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_pipeline_lor, labels=pipeline.classes_))
cm_lor.plot()
#plt.show()
print("===================================")
# ROC-AUC Curve

df['diagnosis'] = le.fit_transform(df['diagnosis'])
y_prob = pipeline.predict_proba(x_test)[:, 1]
print("ROC-AUC :", roc_auc_score(y_test, y_prob))
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc= auc(fpr, tpr)
print("AUC :", roc_auc*100)
plt.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
#plt.show()

print("===================================")    
print("===================== DECEMBER - 13 =====================")



pipeline_nb = Pipeline([
    ('scaler', StandardScaler()),
    ('gnb', GaussianNB())
])
pipeline_nb.fit(x_train, y_train)
y_pred_nb = pipeline_nb.predict(x_test)

print("==========================================")
print("Naive Bayes Classifier Results:")

y_pred_train_gnb = pipeline_nb.predict(x_train)
print("Training Accuracy:", accuracy_score(y_train, y_pred_train_gnb))
print("Testing Accuracy:", accuracy_score(y_test, y_pred_nb))

y_pred_test_gnb = pipeline_nb.predict(x_test)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_test_gnb))

print("==========================================")
print("Classification Report:\n", classification_report(y_test, y_pred_test_gnb))
print("==========================================")
print("Precision M :", precision_score(y_test, y_pred_test_gnb))
print("Precision B :", precision_score(y_test, y_pred_test_gnb,  pos_label=1))
print("Recall M :", recall_score(y_test, y_pred_test_gnb,  pos_label=0))
print("Recall B :", recall_score(y_test, y_pred_test_gnb,  pos_label=1))
print("F1 Score M :", f1_score(y_test, y_pred_test_gnb,  pos_label=0))
print("F1 Score B :", f1_score(y_test, y_pred_test_gnb,  pos_label=1))	
print("==========================================")

cm_gnb=ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test_gnb, labels=pipeline_nb.classes_), 
							  display_labels=['Benign', 'Malignant'])
cm_gnb.plot()
#plt.show()

print("==========================================")
# ROC-AUC Curve for Naive Bayes
y_prob_gnb = pipeline_nb.predict_proba(x_test)[:, 1]
print("ROC-AUC (Naive Bayes):", roc_auc_score(y_test, y_prob_gnb))
fpr_gnb, tpr_gnb, thresholds_gnb = roc_curve(y_test, y_prob_gnb)
roc_auc_gnb = auc(fpr_gnb, tpr_gnb)
plt.plot(fpr_gnb, tpr_gnb, color='green', label='ROC curve (area = %0.2f)' % roc_auc_gnb)
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Naive Bayes')
#plt.show()

print("==========================================")
# Multinomial Naive Bayes
pipeline_mnb = Pipeline([
    ('scaler', MinMaxScaler()),
    ('mnb', MultinomialNB())
])
pipeline_mnb.fit(x_train, y_train)
y_pred_mnb = pipeline_mnb.predict(x_test)
print("Multinomial Naive Bayes Classifier Results:")    
print("Testing Accuracy:", accuracy_score(y_test, y_pred_mnb))
print("Precision M :", precision_score(y_test, y_pred_mnb))
print("Precision B :", precision_score(y_test, y_pred_mnb,  pos_label=1))
print("Recall M :", recall_score(y_test, y_pred_mnb,  pos_label=0))
print("Recall B :", recall_score(y_test, y_pred_mnb,  pos_label=1))
print("F1 Score M :", f1_score(y_test, y_pred_mnb,  pos_label=0))
print("F1 Score B :", f1_score(y_test, y_pred_mnb,  pos_label=1))
print("==========================================")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_mnb))
print("Classification Report:\n", classification_report(y_test, y_pred_mnb))
print("==========================================")

# Bernoulli Naive Bayes
pipeline_bnb = Pipeline([
    ('scaler', StandardScaler()),
    ('bnb', BernoulliNB())
])
pipeline_bnb.fit(x_train, y_train)
y_pred_bnb = pipeline_bnb.predict(x_test)
print("Bernoulli Naive Bayes Classifier Results:")    
print("Testing Accuracy:", accuracy_score(y_test, y_pred_bnb))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_bnb))
print("Classification Report:\n", classification_report(y_test, y_pred_bnb))
print("==========================================")

y_pred_test_prob_mnb = pipeline_mnb.predict_proba(x_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test,y_pred_test_prob_mnb)
roc_auc = auc(fpr,tpr) # x,y ->fpr,tpr
plt.plot(fpr, tpr, color='orange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--', lw=2, color='grey')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
#plt.show()


print("===================================")
#K-nearest Neighbors (KNN) Classifier
from sklearn.neighbors import KNeighborsClassifier

pipeline_knn = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier(n_neighbors=5))
])
pipeline_knn.fit(x_train, y_train)
y_pred_knn = pipeline_knn.predict(x_test)
print("K-Nearest Neighbors Classifier Results:")
print("Testing Accuracy:", accuracy_score(y_test, y_pred_knn))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))
print("Classification Report:\n", classification_report(y_test, y_pred_knn))
print("==========================================")

from sklearn.model_selection import GridSearchCV
param_grid = {'knn__n_neighbors': list(range(1, 21))}
grid_search = GridSearchCV(pipeline_knn, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_train, y_train)
print("Best parameters from Grid Search:", grid_search.best_params_)
print("Best cross-validation accuracy:", grid_search.best_score_)
print("==========================================") 

# Evaluate the best model from Grid Search
optimal_k = grid_search.best_params_['knn__n_neighbors']
print(f"Evaluating KNN with optimal k={optimal_k}")
print("Optimal score on test set:", grid_search.score(x_test, y_test))
print("===================================")

plt.figure(figsize=(10, 6))
plt.plot(range(1, 21), grid_search.cv_results_['mean_test_score'], marker='*')
plt.title('KNN Hyperparameter Tuning')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Mean Cross-Validation Accuracy')
plt.show()


